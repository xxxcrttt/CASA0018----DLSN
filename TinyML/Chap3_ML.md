# 第三章 机器学习 Machine Learning

## 3.1 什么是机器学习
* 机器学习是一种使用计算机基于过去的观察结果来预测事件的技术    
* 设计算法，接收输入，应用规则最终返回一个输出。  
* 根据提供的数据，通过 训练(training) 来建立系统的 模型(model),     
  该模型是一种计算机程序，通过模型来处理数据并进行预测 -- 推断(inferece)   
* 深度学习(deep learning): 一个由模拟神经元组成的 网络(network) 会被训练来模拟各种输出和输出的关系。不同的 架构(architecture) 或模拟神经元的排列适用于不同类型的任务。


## 3.2 Deep Learning Workflow

### 1. 选择一个目标 
预测prediction ---- 决定收集数据 collect data ---- 使用模型 model architecture

### 2. 收集数据集 
* 最好仅仅使用与解决问题相关的信息 
* 经验法则: 数据越多越好！
* 数据被记录为一个 时间序列(time series) 的集合，代表着一个系列定期收集的数据 
#### 标记数据 label
* 数据与类别相关联的过程 -- label class
* 在训练过程中为算法提供表明含义的数据 --  监督学习(supervised learning)


### 3. 设计模型架构 
* 考虑要解决的问题类型，可以访问的数据类型，以及将数据输入之前的转换数据的方式
* 运行模型的设备约束 -- 模型的大小取决于包含的神经元数量以及它们之间的连接方式
* 首先训练一个带有几层神经元的简单模型，然后以 迭代(iteration) 的方式完善架构，直到获得有用的结果
#### 维度 Dimensions
* DL 接受输入并以张量的形式生成输出 
* 张量的结构被称为 形状(shape), 可以有多个维度     
  **向量 vector**: 1D tensor    
  是一个数字列表，类似于数组；
  ```
  形状为(5, )的向量: [42 35 8 643 7]
  ```
  **矩阵 matrix**: 2D     
  是一个二维张量，类似于二维数组; 
  ```
  形状为(3, 3)的矩阵，包含了 3 个有着 3 个数字的向量: [[1 2 3] [4 5 6] [7 8 9]]
  ```
  **高维张量 higher-dimensional tensor** 
  任何具有两个以上维度的形状都是张量
  ```
  形状为(2, 3, 3)的三维张量，包含了两个形状为(3, 3)的矩阵：
   [[[10 20 30]
     [40 50 60]
     [70 80 90]]
    [[11 21 31]
     [41 51 61]
     [71 81 91]]]
   ```
  **标量 scalar** 
  单个数字被称为标量，是 0D 张量，如 42 
#### 从数据生成特征 Generating 
* 特征(feature) 指: 训练模型所依据的特定类型的信息，不同的模型需要使用不同的特征进行训练    
* **窗口化 windowing** 
* **标准化 normalization**    
  输入神经网络的数据将采用张量的形式，此张量被浮点值(floating-point)填充，表示小数，需要对其进行标准化，使得它们都在相似的范围内     
  其中一种方法是：计算数据集中每个特征的平均值，value - mean，压缩至更接近 0 的值得效果
  
### 4. 训练模型 training 
* 训练是一个模型学习为给定的一组输入产生正确输出的过程。涉及为模型提供训练数据，并对其进行微调，直到作出尽可能准确的预测为止   
* 模型是有层次排列的数组表示的模拟神经元 -- 权重(weight) 和 偏差(bias) -- 参数(parameter) -- y = wx + b
* 当数据被输入网络时，网络将通过连续的数学运算进行转换，模型的输出是对输入执行这些数学运算的结果。
* weight 始于随机值，bias 通常始于 0. 
* 在训练过程中，批次(batch) 的数据被输入模型中，并将模型的输出与期望的输出比较    
* **反向传播(backpropagation)** 会逐步调整权重和偏差，以 epoch 为单位来决定是否停止训练  
* **收敛(converge)**: 当模型的表现停止改善时，通常会停止训练；当模型能够作出准确预测时，认为时收敛的     
* 模型表现指标为: 损失(loss) + 准确率(accuracy) -- 随着准确率不断提升，损失逐渐减少，直到模型不再改进     
  **损失(loss)**: 提供数字估算值，用于估计模型和预期答案的距离        
  **准备率(accuracy)**: 模型选择了正确预测的次数的百分比     
* **超参数(hyperparameter)**: 为了提高模型的表现，可以改变模型的架构，调整用于建立模型的各种值，并调整训练过程

#### 欠拟合和过拟合 overfitting & underfitting
* 神经网络会学习如何将其行为和数据中识别出的模式进行拟合，若模型拟合正确，将为给定的一组输入产生正确的输出  
  **欠拟合**: 无法学习到足以表示这些模式的方法，无法做出良好的预测 -- 比如: 架构太小而难以使用系统的复杂性，或没有足够多的数据对模型进行训练       
  **过拟合**: 过好的理解了训练数据，能够准确预测细节但无法将学习结果*泛化* 在未见过的数据上 -- 减小模型的大小，正则化

#### 正则化和数据增强 Regularization and Data Augmentation   
**正则化**: 涉及对模型进行约束，以防止模型完整的记住训练过程中输入的数据    
L1 & L2: 调整训练过程中使用的算法, 以惩罚容易过拟合的复杂模型    
dropout: 在训练过程中随机的切断神经元之间的连接     
**数据增强**: 人为的扩展训练数据集的方法，为每个训练输入创建多个附加版本，每个版本的转换保留了其原本的含义，但是改变了数据的确切构成    

#### 训练，验证和测试 train，validation and test 
* 使用训练中从未使用的新数据来验证模型，典型的划分为 60% -- 20% -- 20%     

### 5. 转换模型 TensorFlow
* TensorFlow 模型本质上是一组指令，告诉解释器(interpreter)如何转换数据以产生输出 
* 提供了一个解释器和附带的工具，可以在小型低能耗设备上运行 -- TensorFlow Lite 

### 6. 运行推断(run inference)

### 7. 评估和故障排除
